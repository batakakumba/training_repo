{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "twenty_train = pd.read_csv('train.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "Test = pd.read_csv('test.csv', sep=',', encoding='utf-8')\n",
    "Test=Test[' Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8543824211501703 +/- 0.006707519728383528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96881403688524592"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import ensemble\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 6),analyzer='char_wb')),#ngram_range=(3, 6),max_df=0.5, analyzer='char' 0.8418936775195723 +/- 0.008995447490375489\n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     #('clf',LinearSVC(penalty=\"l2\",loss='squared_hinge')),\n",
    "                     ('clf', SGDClassifier(loss='modified_huber', random_state=42,max_iter=200)),\n",
    "                     #('clf',PassiveAggressiveClassifier(C=0.5602439092715068)),\n",
    "                      #('clf', MultinomialNB(alpha=0.33921084703630489,fit_prior=False)),\n",
    "                      #  ('clf',BernoulliNB(alpha=0.13420609458144628)),#0.8429193534216901 +/- 0.009888828863667175\n",
    "                     #('clf',RandomForestClassifier(n_estimators=100)),#0.7234268982515644 +/- 0.01047402145804584\n",
    "                     \n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.Text, twenty_train.Label)  \n",
    "\n",
    "predicted = text_clf.predict(Test) \n",
    "\n",
    "score = cross_val_score(text_clf, twenty_train.Text, twenty_train.Label, scoring='accuracy', cv=8, n_jobs=-1)\n",
    "\n",
    "print(\"{} +/- {}\".format(score.mean(), score.std()))\n",
    "\n",
    "predicted_train1 = text_clf.predict(twenty_train.Text)\n",
    "np.mean(predicted_train1 == twenty_train.Label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "maxdf 05 0.8539984622177896 +/- 0.0074228040954732386\n",
    "0.97060706967213117\n",
    "\n",
    "\n",
    "0.8543181212463643 +/- 0.007143552490318967\n",
    "0.96849385245901642\n",
    "\n",
    "\n",
    "\n",
    "0.8435585739194156 +/- 0.007756685897387785\n",
    "\n",
    "\n",
    "\n",
    "Out[89]:\n",
    "\n",
    "0.99314805327868849\n",
    "\n",
    "\n",
    "0.8413165596269867 +/- 0.007451230540229675\n",
    "\n",
    "0.8418936775195723 +/- 0.008995447490375489\n",
    "0.95600665983606559\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порпробуем bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-f1dd8b3d00ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtfidf_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_t\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mX_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.13420609458144628\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\install\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    888\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m                                                        max_features)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\install\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[0;32m    772\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[0;32m    773\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    " #                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "  #                   ('clf', SGDClassifier(loss='squared_hinge', penalty='elasticnet',\n",
    "  #                                         alpha=0.001, l1_ratio=0,random_state=42)),\n",
    "#])\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True,norm='l2')\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1),max_df=0.95,min_df=0.5, analyzer='word')\n",
    "X_t= count_vect.fit_transform(twenty_train.Text)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_t)\n",
    "text_clf=BernoulliNB(alpha=0.13420609458144628)\n",
    "bagging = BaggingClassifier(text_clf,n_estimators=100)\n",
    "bagging.fit(X_train_tfidf, twenty_train.Label)  \n",
    "\n",
    "X_new_counts = count_vect.transform(Test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = bagging.predict(X_new_tfidf)     \n",
    "\n",
    "score = cross_val_score(bagging, X_train_tfidf, twenty_train.Label, scoring='accuracy', cv=8, n_jobs=-1)\n",
    "\n",
    "print(\"{} +/- {}\".format(score.mean(), score.std()))\n",
    "\n",
    "np.mean(predicted_train1 == twenty_train.Label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "0.8395897617311174 +/- 0.011688974709492388\n",
    "\n",
    "\n",
    "0.8392695444822916 +/- 0.011801866273057224\n",
    "0.95306096311475408\n",
    "\n",
    "0.8303053628048254 +/- 0.013890627835110471\n",
    "0.93205686475409832\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\install\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8415751029504578 +/- 0.009808439506498266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95043545081967218"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    " #                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "  #                   ('clf', SGDClassifier(loss='squared_hinge', penalty='elasticnet',\n",
    "  #                                         alpha=0.001, l1_ratio=0,random_state=42)),\n",
    "#])\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True,norm='l2')\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1),max_df=0.5, analyzer='word')\n",
    "clf= MultinomialNB(alpha=0.33921084703630489,fit_prior=False)\n",
    "X_t= count_vect.fit_transform(twenty_train.Text)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_t)\n",
    "text_clf=BernoulliNB(alpha=0.13420609458144628)\n",
    "clf2=SGDClassifier(loss='log', penalty='elasticnet',alpha=0.001, l1_ratio=0,random_state=42,n_iter=100)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf), ('knn', text_clf),('ss', clf2)], voting='hard')\n",
    "eclf = eclf.fit( X_train_tfidf, twenty_train.Label)\n",
    "\n",
    "X_new_counts = count_vect.transform(Test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = eclf.predict(X_new_tfidf)     \n",
    "\n",
    "score = cross_val_score(eclf, X_train_tfidf, twenty_train.Label, scoring='accuracy', cv=8, n_jobs=-1)\n",
    "\n",
    "print(\"{} +/- {}\".format(score.mean(), score.std()))\n",
    "\n",
    "np.mean(predicted_train1 == twenty_train.Label)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор параметров через GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__alpha', 'clf__average', 'clf__class_weight', 'clf__epsilon', 'clf__eta0', 'clf__fit_intercept', 'clf__l1_ratio', 'clf__learning_rate', 'clf__loss', 'clf__max_iter', 'clf__n_iter', 'clf__n_jobs', 'clf__penalty', 'clf__power_t', 'clf__random_state', 'clf__shuffle', 'clf__tol', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__max_df': (0.9,0.91),\n",
    "    'vect__min_df': (0.105,0.101),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__norm': (None,'l1','l2'),\n",
    "    #'clf__loss' :  ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron','squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive')\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.Text, twenty_train.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect__max_df: 0.9\n",
      "vect__min_df: 0.101\n"
     ]
    }
   ],
   "source": [
    "gs_clf.best_score_                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор параметров через RandomezedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 6), preprocessor=None, stop_words=None,\n",
       "        st...ty='l2', power_t=0.5, random_state=42,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False))]),\n",
       "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'vect__max_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000006660CF0F60>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring=None, verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "parameters = {\n",
    "             'vect__max_df': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha valuesmodel = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=text_clf, param_distributions=parameters, n_iter=100)\n",
    "rsearch.fit(twenty_train.Text, twenty_train.Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect__max_df: 0.87569318668869223\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, rsearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Выведем результат в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted,columns=['Prediction'])\n",
    "df.index += 1 \n",
    "df.to_csv('pred.csv',  index_label='Id', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
